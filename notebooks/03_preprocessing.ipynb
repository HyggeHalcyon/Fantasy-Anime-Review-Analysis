{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-16T14:41:22.613521Z",
     "start_time": "2025-04-16T14:41:21.017499Z"
    }
   },
   "source": [
    "import nltk\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "import datetime\n",
    "import contractions\n",
    "\n",
    "nltk_lib_path = \"../libs/nltk_data\"\n",
    "\n",
    "os.makedirs(nltk_lib_path, exist_ok=True)\n",
    "\n",
    "nltk.data.path.append(nltk_lib_path)\n",
    "\n",
    "nltk.download('stopwords', download_dir=nltk_lib_path)\n",
    "nltk.download('vader_lexicon', download_dir=nltk_lib_path)\n",
    "# nltk.download('wordnet', download_dir=nltk_data_path)\n",
    "# nltk.download('omw-1.4', download_dir=nltk_data_path)\n",
    "# nltk.download('punkt_tab', download_dir=nltk_data_path)\n",
    "\n",
    "print(f\"NLTK Data Path: {nltk.data.path}\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to ../libs/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package vader_lexicon to ../libs/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Data Path: ['/home/miellilas/nltk_data', '/home/miellilas/Documents/pba/myanimelist/venv/nltk_data', '/home/miellilas/Documents/pba/myanimelist/venv/share/nltk_data', '/home/miellilas/Documents/pba/myanimelist/venv/lib/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data', '../libs/nltk_data']\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T14:41:25.049794Z",
     "start_time": "2025-04-16T14:41:24.352438Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "nltk_data_path = \"../libs/nltk_data\"\n",
    "nltk.data.path.append(nltk_data_path)"
   ],
   "id": "6c3bf3c2562335c7",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T14:51:57.588062Z",
     "start_time": "2025-04-16T14:51:57.561618Z"
    }
   },
   "cell_type": "code",
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "negation_words = {\"no\", \"not\", \"nor\", \"never\", \"n't\", \"dont\"}\n",
    "stop_words = stop_words - negation_words\n",
    "\n",
    "# Optional dictionary for post-processing corrections (lemmatization errors, slang, etc.)\n",
    "post_lemmatization_corrections = {\n",
    "    \"datum\": \"data\",\n",
    "    \"cannot\": \"can_not\",\n",
    "    \"dont\": \"do_not\",\n",
    "    \"doesnt\": \"does_not\",\n",
    "    \"wont\": \"will_not\",\n",
    "    \"cant\": \"can_not\",\n",
    "    \"isnt\": \"is_not\",\n",
    "    \"wasnt\": \"was_not\",\n",
    "    \"arent\": \"are_not\"\n",
    "}\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        return \"\"\n",
    "\n",
    "    # Expand contractions safely\n",
    "    try:\n",
    "        text = contractions.fix(text)\n",
    "    except Exception as e:\n",
    "        print(f\"[Warning] Contractions expansion failed for: {text}\\nError: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove URLs, emails\n",
    "    text = re.sub(r\"http\\S+|www\\S+|\\S+@\\S+\", \"\", text)\n",
    "\n",
    "    # Remove mentions\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)\n",
    "\n",
    "    # Replace exclamations with a tag (optional emphasis cue)\n",
    "    text = re.sub(r\"!\", \" EXCLAMATION \", text)\n",
    "\n",
    "    # Remove non-alpha characters except apostrophes\n",
    "    text = re.sub(r\"[^a-z'\\s]\", \"\", text)\n",
    "\n",
    "    # Tokenize & lemmatize\n",
    "    try:\n",
    "        doc = nlp(text)\n",
    "    except Exception as e:\n",
    "        print(f\"[Warning] spaCy failed to process: {text}\\nError: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "    tokens = []\n",
    "    skip_next = False\n",
    "\n",
    "    for i, token in enumerate(doc):\n",
    "        if skip_next:\n",
    "            skip_next = False\n",
    "            continue\n",
    "\n",
    "        lemma = token.lemma_.lower()\n",
    "        lemma = post_lemmatization_corrections.get(lemma, lemma)\n",
    "\n",
    "        # Preserve negation + meaningful word (negation tagging)\n",
    "        if lemma in negation_words and i + 1 < len(doc):\n",
    "            next_token = doc[i + 1]\n",
    "            if next_token.pos_ in {\"ADJ\", \"VERB\", \"ADV\", \"NOUN\"}:\n",
    "                next_lemma = next_token.lemma_.lower()\n",
    "                next_lemma = post_lemmatization_corrections.get(next_lemma, next_lemma)\n",
    "                tokens.append(f\"{lemma}_{next_lemma}\")\n",
    "                skip_next = True\n",
    "                continue\n",
    "            else:\n",
    "                tokens.append(lemma)\n",
    "        elif lemma not in stop_words and token.is_alpha and len(lemma) > 1:\n",
    "            tokens.append(lemma)\n",
    "\n",
    "    return \" \".join(tokens)\n"
   ],
   "id": "3b3abdac5999367c",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T15:04:06.219961Z",
     "start_time": "2025-04-16T14:51:59.763377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(\"../data/raw/top_150_fantasy_reviews.csv\")\n",
    "\n",
    "df[\"processed_review\"] = df[\"Review\"].apply(preprocess_text)\n",
    "\n",
    "df = df[df[\"processed_review\"].str.strip().astype(bool)]\n",
    "\n",
    "date_today = datetime.datetime.today().strftime(\"%Y%m%d_%H%M\")\n",
    "df.to_csv(f\"../data/processed/{date_today}_df_cleaned.csv\", index=False)"
   ],
   "id": "4d937ab6b5162948",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] Contractions expansion failed for: this was the most beatifull OVA i've ever seen so far. But not the other one , This one has it's own plot , this one has it's own emotional transitions . Well \n",
      "\r\n",
      "Story 9      Art 10     Sound  9     Character 8     Enjoyment 8     Overall 8.5\n",
      "\n",
      "\r\n",
      "But , not the other one. This Show has charmed me To it's plot. May Spoilers...\n",
      "\r\n",
      "A girl with red hair,  A mysterious air, A smooth scenario, and a lot to say about a little show. But unfortunately i\n",
      "                  ...\n",
      "don't have much time for this review. İf you interested in these cases, then check the Ova version. I think this is the way worth if we compare the other one. And again i think This one is more original than the other. So that was a review Have a time can be called good for you\n",
      "don't have much time for this review. İf you interested in these cases, then check the Ova version. I think this is the way worth if we compare the other one. And again i think This one is more original than the other. So that was a review Have a time can be called good for you\n",
      "Error: string index out of range\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T15:04:06.342637Z",
     "start_time": "2025-04-16T15:04:06.326273Z"
    }
   },
   "cell_type": "code",
   "source": "df[\"processed_review\"].head(3)",
   "id": "fc7e6195074472fc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    life short even bother someone live thousand y...\n",
       "1    feel catered feel like eternity since give phe...\n",
       "2    style frieren not_have unique style way feel l...\n",
       "Name: processed_review, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
